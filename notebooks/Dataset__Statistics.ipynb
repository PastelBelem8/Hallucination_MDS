{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11c0a77-ad3e-4a70-a738-43abd5785516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import tiktoken\n",
    "encoding__ = tiktoken.get_encoding(\"cl100k_base\")\n",
    "num_tokens = lambda x: len(encoding__.encode(x))\n",
    "\n",
    "from collections import defaultdict\n",
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "\n",
    "\n",
    "import os, sys; sys.path.append(\"../src\")\n",
    "import utils_io\n",
    "\n",
    "\n",
    "def read_jsons(parent_dir: str):\n",
    "    data = {}\n",
    "    for fname in sorted(os.listdir(parent_dir)):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        data[fname] = utils_io.read_json(f\"{parent_dir}/{fname}\")\n",
    "        print(len(data[fname][\"assignments\"]))\n",
    "    assert len(data) > 0, f\"Empty {parent_dir}\"\n",
    "    return data\n",
    "\n",
    "def print_values(data_dict, msg):\n",
    "    print(f\"\\t{msg}\\n\", \"-\"*len(msg) + \"-----------\")\n",
    "    \n",
    "    all_values = np.hstack(list(data_dict.values()))\n",
    "    print(f\"Total:\\t {np.mean(all_values):.1f} (± {np.std(all_values):.1f})\")\n",
    "    print(f\"Total count:\", len(all_values))\n",
    "    \n",
    "    for fname, fvalues in data_dict.items():\n",
    "        print(f\"-> {fname}:\\t {np.mean(fvalues):.1f} (± {np.std(fvalues):.1f})\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "def document_length(data):\n",
    "    results = []\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        docs_uuids = assignment[\"docs_uuids\"]\n",
    "        docs_texts = [data[\"documents\"][uuid] for uuid in docs_uuids]\n",
    "        docs_texts = [doc[\"document_text\"] for doc in docs_texts]\n",
    "        \n",
    "        doc_length = [num_tokens(t) for t in docs_texts]    \n",
    "        doc_length = np.sum(doc_length)\n",
    "        results.append(doc_length)\n",
    "    return results\n",
    "\n",
    "def all_subtopics(data):\n",
    "    results = []\n",
    "    \n",
    "    for assignment in data[\"assignments\"]:\n",
    "        all_uuids = assignment[\"all_subtopics_uuids\"]\n",
    "        assert len(np.unique(all_uuids)) == len(all_uuids)\n",
    "        results.append(len(all_uuids))\n",
    "    return results\n",
    "\n",
    "def shared_subtopics(data):\n",
    "    results = []\n",
    "    \n",
    "    for assignment in data[\"assignments\"]:\n",
    "        all_shared_subtopics = [uuid for uuid, docs in assignment[\"subtopics_to_documents\"].items() if len(docs) > 1]\n",
    "        assert len(np.unique(all_shared_subtopics)) == len(all_shared_subtopics)\n",
    "        results.append(len(all_shared_subtopics))\n",
    "    return results\n",
    "\n",
    "def all_insights(data):\n",
    "    results = []\n",
    "\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        all_uuids = assignment[\"all_insights_uuids\"]\n",
    "        assert len(np.unique(all_uuids)) == len(all_uuids)\n",
    "        results.append(len(all_uuids))\n",
    "    return results\n",
    "\n",
    "def subtopic_insights(data, is_adversarial=False):\n",
    "    results = []\n",
    "\n",
    "    uuid2insights = data[\"insights\"]\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        all_uuids = assignment[\"all_insights_uuids\"]\n",
    "\n",
    "        subtopic_uuid = assignment[\"subtopic_uuid\"]\n",
    "        insights = [uuid2insights[uuid] for uuid in all_uuids]\n",
    "        insights = [ins for ins in insights if ins[\"subtopic_id\"] == subtopic_uuid]\n",
    "\n",
    "        assert (is_adversarial and len(insights) == 0) or len(insights) > 0\n",
    "        results.append(len(insights))\n",
    "    return results\n",
    "\n",
    "\n",
    "def shared_subtopic_insights(data, is_adversarial=False):\n",
    "    results = []\n",
    "\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        uuids = assignment[\"ground_truth_insights_uuids\"]\n",
    "        assert (is_adversarial and len(uuids) == 0) or len(np.unique(uuids)) == len(uuids)\n",
    "        results.append(len(uuids))\n",
    "    return results\n",
    "\n",
    "\n",
    "def shared_insights(data, is_adversarial=False):\n",
    "    results = []\n",
    "\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        uuids = set(assignment[\"some_shared_insights_uuids\"] + assignment[\"all_shared_insights_uuids\"])\n",
    "        results.append(len(uuids))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7ad1f-55d4-4af5-8b65-07ca0a69419a",
   "metadata": {},
   "source": [
    "# Statistics combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fb1fd8-9d3c-490e-86ec-37d378bc29b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "60\n",
      "57\n",
      "71\n",
      "85\n",
      "\tAverage file token length\n",
      " ------------------------------------\n",
      "Total:\t 2054.6 (± 235.2)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 2034.7 (± 209.0)\n",
      "-> topic_conv2__300:\t 1949.9 (± 223.2)\n",
      "-> topic_conv3__300:\t 2101.2 (± 227.1)\n",
      "-> topic_conv4__300:\t 2004.8 (± 219.9)\n",
      "-> topic_conv5__300:\t 2154.9 (± 235.1)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of subtopics in documents\n",
      " ---------------------------------------------------\n",
      "Total:\t 2.9 (± 0.3)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 2.9 (± 0.2)\n",
      "-> topic_conv2__300:\t 2.9 (± 0.3)\n",
      "-> topic_conv3__300:\t 2.9 (± 0.3)\n",
      "-> topic_conv4__300:\t 2.9 (± 0.3)\n",
      "-> topic_conv5__300:\t 2.9 (± 0.3)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of shared subtopics in documents\n",
      " ----------------------------------------------------------\n",
      "Total:\t 1.1 (± 0.3)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 1.1 (± 0.2)\n",
      "-> topic_conv2__300:\t 1.1 (± 0.3)\n",
      "-> topic_conv3__300:\t 1.1 (± 0.3)\n",
      "-> topic_conv4__300:\t 1.1 (± 0.3)\n",
      "-> topic_conv5__300:\t 1.1 (± 0.3)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of insights in documents\n",
      " --------------------------------------------------\n",
      "Total:\t 5.2 (± 0.7)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 5.3 (± 0.7)\n",
      "-> topic_conv2__300:\t 5.0 (± 0.7)\n",
      "-> topic_conv3__300:\t 5.2 (± 0.7)\n",
      "-> topic_conv4__300:\t 5.4 (± 0.6)\n",
      "-> topic_conv5__300:\t 5.2 (± 0.7)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of subtopic insights\n",
      " ----------------------------------------------\n",
      "Total:\t 2.0 (± 0.0)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv2__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv3__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv4__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv5__300:\t 2.0 (± 0.0)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of shared insights in documents\n",
      " ---------------------------------------------------------\n",
      "Total:\t 2.0 (± 0.2)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 2.0 (± 0.1)\n",
      "-> topic_conv2__300:\t 2.0 (± 0.2)\n",
      "-> topic_conv3__300:\t 2.0 (± 0.2)\n",
      "-> topic_conv4__300:\t 2.0 (± 0.1)\n",
      "-> topic_conv5__300:\t 2.0 (± 0.2)\n",
      "\n",
      "\n",
      "\n",
      "\tAverage number of shared subtopic insights in documents\n",
      " ------------------------------------------------------------------\n",
      "Total:\t 2.0 (± 0.0)\n",
      "Total count: 341\n",
      "-> topic_conv1__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv2__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv3__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv4__300:\t 2.0 (± 0.0)\n",
      "-> topic_conv5__300:\t 2.0 (± 0.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../data/SummHay_conv/preprocessed/some_shared/combinations-2/\"\n",
    "dataset = read_jsons(base_dir)\n",
    "dataset = {fname.replace(\".json\", \"\"): data for fname, data in dataset.items()}\n",
    "is_adversarial = \"adversarial\" in base_dir\n",
    "\n",
    "#   Dataset length\n",
    "# ------------------\n",
    "dataset_lens = {fname: document_length(data) for fname, data in dataset.items()}\n",
    "print_values(dataset_lens, \"Average file token length\")\n",
    "\n",
    "# ---------------------------\n",
    "#  Number of all subtopics\n",
    "# ---------------------------\n",
    "dataset_all_subtopics = {fname: all_subtopics(data) for fname, data in dataset.items()}\n",
    "print_values(dataset_all_subtopics, \"Average number of subtopics in documents\")\n",
    "\n",
    "# ---------------------------\n",
    "#  Number of shared subtopics\n",
    "# ---------------------------\n",
    "dataset_shared_subtopics = {fname: shared_subtopics(data) for fname, data in dataset.items()}\n",
    "print_values(dataset_shared_subtopics, \"Average number of shared subtopics in documents\")\n",
    "\n",
    "# ---------------------------\n",
    "#  Number of all insights\n",
    "# ---------------------------\n",
    "dataset_all_insights = {fname: all_insights(data) for fname, data in dataset.items()}\n",
    "print_values(dataset_all_insights, \"Average number of insights in documents\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "#  Number of all insights related to the subtopic\n",
    "# ------------------------------------------------------\n",
    "dataset_all_subtopic_insights = {fname: subtopic_insights(data, is_adversarial) for fname, data in dataset.items()}\n",
    "print_values(dataset_all_subtopic_insights, \"Average number of subtopic insights\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#  Number of shared insights regardless of subtopic\n",
    "# ---------------------------------------\n",
    "dataset_shared_insights = {fname: shared_insights(data, is_adversarial) for fname, data in dataset.items()}\n",
    "print_values(dataset_shared_insights, \"Average number of shared insights in documents\")\n",
    "\n",
    "# ---------------------------------------\n",
    "#  Number of shared insights by subtopic\n",
    "# ---------------------------------------\n",
    "dataset_shared_subtopic_insights = {fname: shared_subtopic_insights(data, is_adversarial) for fname, data in dataset.items()}\n",
    "print_values(dataset_shared_subtopic_insights, \"Average number of shared subtopic insights in documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb99513-35a8-4386-9345-754946ed38b8",
   "metadata": {},
   "source": [
    "# Statistics of Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5105dda4-dece-4e00-bf34-0eab11059287",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'parse_responses_' from 'eval_summaries' (/Users/catarinabelem/Desktop/Projects/hallucination-multi-doc-summarization/notebooks/../src/eval_summaries.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m; sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval_summaries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_responses_\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresponse_length\u001b[39m(data):\n\u001b[1;32m      5\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'parse_responses_' from 'eval_summaries' (/Users/catarinabelem/Desktop/Projects/hallucination-multi-doc-summarization/notebooks/../src/eval_summaries.py)"
     ]
    }
   ],
   "source": [
    "import os, sys; sys.path.append(\"../src\")\n",
    "from eval_summaries import parse_responses_\n",
    "\n",
    "def response_length(data):\n",
    "    results = []\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        results.append(num_tokens(assignment[\"response\"])) \n",
    "    return results\n",
    "\n",
    "def num_insights_in_response(data):\n",
    "    results = []\n",
    "    for assignment in data[\"assignments\"]:\n",
    "        results.append(len(assignment[\"response__parsed\"]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f119aa7-e98e-40a4-b42b-ac224abe8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../outputs/results/SummHay__combinations-4/gpt-3.5-turbo-0125\"\n",
    "results = read_jsons(base_dir)\n",
    "results = {fname.replace(\".json\", \"\"): data for fname, data in results.items()}\n",
    "is_adversarial = \"adversarial\" in base_dir\n",
    "\n",
    "#   Dataset length\n",
    "# ------------------\n",
    "results_lens = {fname: response_length(data) for fname, data in results.items()}\n",
    "print_values(results_lens, \"Average response token length\")\n",
    "\n",
    "# -------------------------------\n",
    "#  Number of predicted insights\n",
    "# -------------------------------\n",
    "[parse_responses_(data[\"assignments\"]) for data in results.values()]\n",
    "results_num_insights = {fname: num_insights_in_response(data) for fname, data in results.items()}\n",
    "print_values(results_num_insights, \"Average number of predicted insights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc6abd-c46d-455e-814b-01aa4ec28a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
